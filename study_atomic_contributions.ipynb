{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f9456fb-d22b-4fd8-b465-d036c5612381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baillifb/anaconda3/envs/GeoMol/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378098133/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import os.path as osp\n",
    "from math import pi as PI\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding, Sequential, Linear, ModuleList\n",
    "import numpy as np\n",
    "\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.data.makedirs import makedirs\n",
    "from torch_geometric.data import download_url, extract_zip, Dataset\n",
    "from torch_geometric.nn import radius_graph, MessagePassing\n",
    "\n",
    "def atomic_forward(self, z, pos, batch=None):\n",
    "    assert z.dim() == 1 and z.dtype == torch.long\n",
    "    batch = torch.zeros_like(z) if batch is None else batch\n",
    "\n",
    "    h = self.embedding(z)\n",
    "\n",
    "    edge_index = radius_graph(pos, r=self.cutoff, batch=batch,\n",
    "                              max_num_neighbors=self.max_num_neighbors)\n",
    "    row, col = edge_index\n",
    "    edge_weight = (pos[row] - pos[col]).norm(dim=-1)\n",
    "    edge_attr = self.distance_expansion(edge_weight)\n",
    "\n",
    "    for interaction in self.interactions:\n",
    "        h = h + interaction(h, edge_index, edge_weight, edge_attr)\n",
    "\n",
    "    h = self.lin1(h)\n",
    "    h = self.act(h)\n",
    "    h = self.lin2(h)\n",
    "\n",
    "    if self.dipole:\n",
    "        # Get center of mass.\n",
    "        mass = self.atomic_mass[z].view(-1, 1)\n",
    "        c = scatter(mass * pos, batch, dim=0) / scatter(mass, batch, dim=0)\n",
    "        h = h * (pos - c.index_select(0, batch))\n",
    "\n",
    "    if not self.dipole and self.mean is not None and self.std is not None:\n",
    "        h = h * self.std + self.mean\n",
    "\n",
    "    if not self.dipole and self.atomref is not None:\n",
    "        h = h + self.atomref(z)\n",
    "\n",
    "    out = scatter(h, batch, dim=0, reduce=self.readout)\n",
    "\n",
    "    if self.dipole:\n",
    "        out = torch.norm(out, dim=-1, keepdim=True)\n",
    "\n",
    "    if self.scale is not None:\n",
    "        out = self.scale * out\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69b12d7-7ba5-43b6-acf4-b86621f69d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16fb9d34-8534-4c7e-a2aa-ef8831b17620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once to preprocess datasets and generate chunks\n",
    "# dataset = ConfEnsembleDataset()\n",
    "# dataset = ConfEnsembleDataset(dataset='platinum') # 16G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc7808fa-0591-48ec-841a-5496eb1055af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbbind_chunks = [filename for filename in os.listdir(os.path.join(data_dir, 'processed')) if filename.startswith('pdbbind')]\n",
    "pdbbind_n_chunks = len(pdbbind_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfb25fe7-ed9c-416b-b572-7c86e87b3e92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28026/1540202021.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scaffold_splits'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'train_smiles_scaffold_split_{iteration}.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_smiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_smiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msmiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_smiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "with open(os.path.join(data_dir, 'scaffold_splits', f'train_smiles_scaffold_split_{iteration}.txt'), 'r') as f :\n",
    "    train_smiles = f.readlines()\n",
    "    train_smiles = [smiles.strip() for smiles in train_smiles]\n",
    "\n",
    "with open(os.path.join(data_dir, 'scaffold_splits', f'val_smiles_scaffold_split_{iteration}.txt'), 'r') as f :\n",
    "    val_smiles = f.readlines()\n",
    "    val_smiles = [smiles.strip() for smiles in val_smiles]\n",
    "\n",
    "with open(os.path.join(data_dir, 'scaffold_splits', f'test_smiles_scaffold_split_{iteration}.txt'), 'r') as f :\n",
    "    test_smiles = f.readlines()\n",
    "    test_smiles = [smiles.strip() for smiles in test_smiles]\n",
    "\n",
    "train_datasets = []\n",
    "val_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "for chunk_number in tqdm(range(pdbbind_n_chunks)) :\n",
    "\n",
    "    dataset = ConfEnsembleDataset(loaded_chunk=chunk_number,\n",
    "                                  smiles_list=train_smiles)\n",
    "    train_datasets.append(dataset)\n",
    "\n",
    "    dataset = ConfEnsembleDataset(loaded_chunk=chunk_number,\n",
    "                                  smiles_list=val_smiles)\n",
    "    val_datasets.append(dataset)\n",
    "\n",
    "    dataset = ConfEnsembleDataset(loaded_chunk=chunk_number,\n",
    "                                  smiles_list=test_smiles)\n",
    "    test_datasets.append(dataset)\n",
    "\n",
    "train_dataset = ConcatDataset(train_datasets)\n",
    "val_dataset = ConcatDataset(val_datasets)\n",
    "test_dataset = ConcatDataset(test_datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
